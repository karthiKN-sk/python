 Artificial Intelligence

-> Generally Deep Learning is a subset of Neural Network
-> Generally Neural Network is a subset of Machine Learning
-> Generally Machine Learning is a subset of Artificial Intelligence


-> Generally All Combined together as Artificial Intelligence

>  Artificial Intelligence             Machine Learning            Neural Network            Deep Learning 
              AI                 >         ML                >       NN             >       DL

 Artificial Intelligence (AI)
   └── Machine Learning (ML)
         └── Neural Networks (NN)
                └── Deep Learning (DL)


        +------------------------+
        | Artificial Intelligence|
        +------------------------+
                    |
        +------------------------+
        |   Machine Learning     |
        +------------------------+
                    |
        +------------------------+
        |   Neural Networks      |
        +------------------------+
                    |
        +------------------------+
        |   Deep Learning        |
        +------------------------+



> Machine Learning 


  1. Supervised Learning -> the algorithm is trained on a labeled dataset, where each input is paired with its corresponding output.(human intervention needs to label the data appropriately)
     1. Classification --> discrete class label (spam or not Spam) -> (Linear Classifier, support vector machines or spms , decision trees, random forests)[Comman eamples of Classification algorithms]
     2. Regression  --> continuous value, such as price or probability -> (Linear regression and logistic regression)  [Comman eamples of Regression algorithms]

  2. Unsupervised learning -> the algorithm is provided unlabeled data and is tasked with finding patterns or relationships within it. The goal of the algorithm is to inherent structures or groups in the data.
     1. Clustering  -> together based on similarities like age or location or spending habits (Clusting is where the algorithm groups similar experiences together)
     2. Association  -> its looks for relationship between variables in the data (often used in market basket analysis like items are often bought together)
     3. Dimensional reduction  -> is used in the pre processing data stage(such as auto encoders remove noise from visual images to improve picture quality)

  3. Semi-supervised Learning -> training dataset with both labeled and unlabled data.(its particularly useful when its difficult to extract relevent features from data when you have a high volume of data)





> Deep Learning + LLM = Generative AI (Field of Generative AI we don't deal with text but we deal with other modalities of media like image, audio and video)



LLM = Large Language Model

        > Creating an LLM = Pretraining + FineTuning

        > PreTraining -> Training on a large, diverse dataset
                        Teach the model general language understanding using a massive, unlabeled corpus.

                        [ Massive Unlabeled Text Data ] 
                                    ↓
                        [ Tokenization (e.g., BPE) ]
                                    ↓
                        [ Transformer Architecture (e.g., GPT, BERT) ]
                                    ↓
                        [ Masked Language Modeling (BERT) or Causal LM (GPT) ]
                                    ↓
                        [ Pretrained LLM ]

        > Data: Wikipedia, books, Common Crawl, etc.
        > Task: Predict the next word (GPT) or masked word (BERT).
        > Output: A general-purpose language model.

        > FineTuning -> Refinement by training on narrower dataset, specfic to a particular task or domain
                        Specialize the model on a specific task or dataset.
                        [ Pretrained LLM ]
                                +
                        [ Task-Specific Labeled Data (e.g., QA, Chat, Summarization) ]
                                ↓
                        [ Continue Training (usually with smaller LR) ] Learning Rate
                                ↓
                        [ Task-Specific LLM ]

        > Chatbot → fine-tune on dialogue data
        > Medical QA → fine-tune on health datasets
        > Coding assistant → fine-tune on GitHub code



 > Pretraining + FineTuning Schematic


            Data                  |          |   has Basic Capabilites      |                |      Pretained LLM trained on labelled dataset     
                                  |          |                              |                | 
        Internet text,books       | Train    |     pretained LLM            |   Train        |       *Classification
        media, research articles  |--------> |   (foundation model)         | ------------>  |       *Summarization
                                  |          |                              | Labelled       |       *Translation
        Raw unlabeled text        |          |  LLM pretrained on unlabeled |  Dataset       |       *Personal Assisstant
        Trillions of words        |          |  text data                   |                |   -> Finetuned LLM

                                            ┌─────────────────────────────┐
                                            │     Massive Text Corpus     │
                                            └────────────┬────────────────┘
                                                        ▼
                                            [ Pretraining Phase ]
                                                        ▼
                                                ┌────────────────────┐
                                                │ General-Purpose LLM│
                                                └────────┬───────────┘
                                                        ▼
                                            [ Fine-Tuning Phase ]
                                                        ▼
                                            ┌────────────────────────────┐
                                            │ LLM for Specific Tasks     │
                                            └────────────────────────────┘











 Transformers vs LLMs

  Feature    |         Transformers                            |        LLMs (Large Language Models)
 Definition  | A model architecture                            | A type of model built (usually) using Transformers
 Role        | The building foundation (like an engine)        | The application (like a car using that engine)
 Purpose     | Used to model sequences (text, audio, etc.)     | Generate, summarize, translate, or understand language
 Examples    | Transformer, BERT, GPT, T5 (architectures)      | GPT-3, GPT-4, PaLM, Claude, LLaMA (trained models)
 Size        | Can be small or large                           | Always large (typically billions of parameters)
 Used in     | Language, vision, audio, time-series            | Primarily language tasks











Retrieval-Augmented Generation (RAG)
> RAG (Retrieval-Augmented Generation) is a super powerful way to combine LLMs with external knowledge sources,
> so your model can generate more accurate, up-to-date, or domain-specific responses—even if that info wasn't in its training data.


> Retrieval-Augmented Generation = LLM + Search Engine / Vector Store

> Instead of generating answers purely from what the model memorized, RAG:
      Retrieves relevant documents from a knowledge base (like a vector DB, PDF, or website),
      Feeds them into the LLM as part of the prompt,
      Then the LLM generates a response based on that info.

         User Query
            ↓
         Retriever  ← (Searches a knowledge base, like a vector DB)
            ↓
         Relevant Chunks (Docs, PDFs, etc.)
            ↓
         Prompt Builder (Combines query + context)
            ↓
         LLM (e.g. GPT, LLaMA, Mistral, etc.)
            ↓
         Generated Answer